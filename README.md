# CMPSC 472 Project 1
# Table of Contents
- [Project Description](#project-description)
- [Structure of Code](#structure-of-code)
- [Instructions on How to Use the Program](#instructions-on-how-to-use-the-program)
- [Verification of Code Functionality](#verification-of-code-functionality)
- [Discussion of Findings](#discussion-of-findings)

**Note:** For this project, I could not get the program to accurately find the top 50 most used words throughout all the files. Instead of doing this, I created an array to store 50 of the most commonly used words in the English language. From here, each process read in one word at a time and that word is compared to all 50 words in the array. If the word matched one of the 50 words in that array, I incremented a counter that keeps track of that word's frequency. In other words, I hardcoded an array containing 50 common words, and these words may not actually be the 50 most common words used throughout all seven files.  

**Note:** This Colab has five programs in it - 1. Seven Child Processes Reading Seven Files. 2. Single Child Process Reading a Single File. 3. No Child Process, Two Threads Created to Read One File. 4. Single Child Process Creating Two Threads to Read a Single File. 5. Seven Processes Creating Two Threads. Programs 1, 2, and 3 all work properly, while programs 4 and 5 do not. Programs 1 and 5 are designed to meet the requirements of the project, and programs 3, and 4 are all experiment programs I created to try to get program 5 to work properly. However, I could not get program 5 to work properly, and I believe the issue is with the threads and child processes communicating with the parent process because the program 3 uses two threads to read the file and works fine. Program 4 introduces a single child process and two threads and does not work properly. To make the performance comparison more even, I created program 2, which uses a single child process to read one file. Program 3 uses no child processes and creates two threads to read the same file. The screenshots of the output and metrics are of programs 2 and 3.

# Project Description
This project involves developing a program to read through seven large files using multiple processes and multiple threads. Each process is assigned to read one file, and each thread is assigned to a specific portion of the file to read through concurrently with other threads. The goal is to identify, count, and display the 50 most-used words with each word displaying its frequency. 

**Program 2 Single Child Process Reading a Single File Explanation:** This program is designed to analyze the frequency of predefined keywords in a text file by utilizing forked child processes and pipes for inter-process communication. The program creates a child process that reads the file, counts occurrences of specific keywords, and sends the results back to the parent process through a pipe. The parent process accumulates the results from the child and displays the final keyword frequencies. Additionally, the program tracks performance metrics, such as total execution time, user and system CPU time used by child processes, and the maximum memory usage during execution.

The program processes a file located at /content/drive/MyDrive/Colab Notebooks/txtfiles/bib stored in an array called file_paths[]. It searches for 50 predefined keywords stored in the keywords[] array. After forking, the child process reads the file, tokenizes it (removes punctuation, newline characters, tab characters, and spaces), and compares each word to the list of keywords. The results are then sent to the parent process via a pipe. Once the parent process receives the results, it sums the frequencies and prints the final counts.

In addition to displaying the keyword frequencies, the program outputs performance statistics. These include the total execution time in microseconds, the amount of time the CPU spent in user mode, the amount of time the CPU spent in kernel mode, and the peak memory usage measured as the maximum resident set size in kilobytes.  

To use the program, first click the 'Run cell' button for the cell that contains the main program. Below this cell, there is another cell containing the code: `%%shell gcc proj1.c -o proj1
./proj1`. This is to compile and run the code in the cell above. Run this cell by clicking the cell's 'Run cell' button. Below the cell that compiles and runs the code is where the output will display.  

**Program 3 No Child Process, Two Threads Created to Read One File Explanation:** 
This program is designed to analyze keyword frequencies in a text file using multithreading. The threads are created using the `pthread_create()` function provided by POSIX. Each of the two threads read the same file, but the first thread only reads the first half of the file while the second thread reads the other half. The program also uses mutexes for synchronization to protect the critical section of the code. This critical section is when the array containing the word frequencies gets updated. This is a critical section because multiple threads iterating through the same array and updating values can cause data to become corrupted.

The main goal of the program is the same as the first program using only child processes: to search for 50 predefined keywords defined in the keywords[] array. The file to be processed is located at /content/drive/MyDrive/Colab Notebooks/txtfiles/bib, and this file path is stored at the same array - the file_paths[] array. The program starts by opening the file and counting the total number of lines. It then divides the file into two halves, assigning each half to a separate thread. Each thread reads its respective section of the file, tokenizes each line into individual words, and compares each word against the predefined keywords, incrementing the corresponding frequency count when a match is found.

A mutex is used to ensure that the threads do not concurrently update the shared frequency array in a way that leads to inconsistencies. After both threads finish execution, the program aggregates the results and displays the frequency counts of the keywords. Additionally, it records and outputs performance statistics such as the total execution time, user CPU time, system CPU time, and the maximum resident set size during execution.

To use the program, first click the 'Run cell' button for the cell that contains the main program. Below this cell, there is another cell containing the code: `%%shell gcc proj1.c -o proj1
./proj1`. This is to compile and run the code in the cell above. Run this cell by clicking the cell's 'Run cell' button. Below the cell that compiles and runs the code is where the output will display.

# Structure of Code  
**Fork Diagram**  
![image](/images/fork_diagram.jpg)  
**Explanation:** The parent process calls fork() to create a child process, and the return value of the fork() function call is a process ID. When that process ID is equal to zero, this indicates the process executing the code is a child process. The child process then runs the code after the fork() call, and in this project, that code creates a file pointer, reads the file, tokenizes the string, searches for words that match one of the keywords from the keywords[] array, updates the frequency of the word that was found, closes the file, then sends the word count to the parent process through a pipe. Meanwhile, once the parent process creates all seven processes, it then moves on to collect all word frequencies from the child processes. It does this by using a for-loop to get values from all seven child processes. The first step inside the for-loop is to close the write end of the pipe. Then using the read() function, the parent reads data coming being sent through the pipe from the child. The parent uses another for-loop to accumulate all word frequencies from each child. After reading in all the frequencies, the parent closes the read end of the pipe and starts over at the top of the for-loop until it reads from all seven children. After it finishes reading, the parent calls uses a while-loop to wait for all children processes to finish execution.  

**Thread Diagram**
![image](/images/thread_diagram.jpg)
**Explanation:** Within the for-loop used to create the seven child processes, the child process then goes on to create two threads. Thread creation is completed through the pthread_create() function that gets called twice to create two threads. Within each pthread_create() function is a function that the thread is supposed to run. In this program, both threads run the same function - the read_file() function. This function allows the thread to read a specific portion of the file. For example, the first thread will read the first half of the file while the second thread will read the second half of the file. The reason for this is so the file can be read in parallel. The threads essentially read the file the same way the single process reads the file where each line of the file is read using fgets(), then the string is tokenized using strtok(). Then a while-loop is used to go through each token. A for-loop is used to compare this token to each of the 50 words stored in the keywords[] array. If the token matches one these words, the frequency of that word is updated. The only difference between the processes and the threads is when the frequencies get updated. A mutex is locked before the frequencies get updated to prevent multiple threads from updating the values in the array used to store word frequencies at once. After the value is updated, the mutex is unlocked to allow another thread to go in and update the frequencies with its own values.  

**Pipe Diagram**   
![image](/images/pipe_diagram.jpg)
**Explanation:** Communication between a child and parent process is enabled through the use of pipes. Each process gets its own set of file descriptors - one for reading from the pipe and one for writing to the pipe. If the child process wants to send data to the parent process, first it must close its own file descriptor for reading. Then using the write() function, it can write its data to the write-end of the pipe situated in the kernel of the operating system. In order for the parent process to read that data, the parent process must first close its own file descriptor for writing. Then using the read() function, it can read the data that the child process has sent. In this program, the data the child processes send are the word frequencies each child has accumulated. Since there are multiple child processes that all need to send their data, the write() function is enclosed in a loop so that all child processes can send their data. The same is true for the parent process - the read() function is enclosed in a loop so that it can read all data from all children. The parent process than accumulates all word frequencies from all seven child processes into a single array called frequencies[]. After reading all data from all children, the frequencies[] array should contain the total number of times each word appeared throughout all seven files that were read.

# Instructions on How to Use the Program
Using this program is easy - all that the user needs to do is just press the 'Run cell' button on the cell containing all the C code. After completing this step, navigate to the cell below the one containing the main program. This cell should contain the code `%%shell gcc proj1.c -o proj1 ./proj1`. Run this cell by clicking the cell's 'Run cell' button. Once this cell finishes running, the output of the program will be displayed this cell.  

This project was completed in Google Colab, so all files that are read are stored in my Google Drive. To get access to these files, I just had to mount Google Drive. Once the drive is mounted, the program handles everything else simply running. It reads all files, identifies keywords stored in the keywords[] array, counts the frequencies of all keywords, then displays each keyword, its frequency, the total execution time, the CPU time used, and the amount of memory used. 

# Verification of Code Functionality
**Note:** Since I could not get the multithreaded program to work properly, I made a simpler program that does work. This simpler program does not create any child processes, but it does create two threads to read a single file. To have a fair comparison, I also modified the non-multithreaded program to create only one child process to read one file. This way I can compare the metrics of the program that uses two threads to read one file and the program that uses a single child process to read the same file.  

**Word Histogram**
![image](/images/keyword_histogram.jpg)  

**Single thread output**  
![image](/images/single-thread-output.jpg)  

**Multithread output**  
![image](/images/multi-thread-output.jpg)  

**Difference in Performance:** For the program that uses only seven child processes and no multithreading, the total execution time was 18.646 milliseconds, whereas the multithreaded program took 16.543 milliseconds to complete its executions. The non-multithreaded program spent 76.560 milliseconds in user mode and 22.121 milliseconds in kernel mode. The multithreaded program spent 54.878 milliseconds in user mode and 21.798 milliseconds in kernel mode. Both programs used a similar amount of memory. The non-multithreaded program used about 20.56MB of memory as opposed to the multithreaded program's 20.91MB.  

**Discussion of Verification Results:** Overall I believe these results make sense. Both programs (programs 2 and 3) output the same words appearing the same number of times, meaning both programs read the same file and get the same results, as expected. Unfortunately, for the programs I could not get to work (programs 4 and 5), the results of the word counts are different after each run. Additionally, the values are extremely high, so there is some sort of communication issue between the threads, child processes, and parent process.

# Discussion of Findings
From this project, I have discovered that the program that does not take advantage of multithreading took longer to run than the program that did take advantage of multithreading. Additionally, the program that did not take advantage of multithreading spent more CPU time in both user and kernel mode. I believe the increased time difference between the non-multithreaded program and the multithreaded program is due to the fact that the non-multithreaded program took more time to execute overall. The multithreaded program took less time to execute overall since it used two threads to read the file in parallel, and since this program took less time overall, this means it also spent less time in the CPU overall, both in user and kernel mode. A non-multithreaded program should take longer to execute overall since the program does not take advantage of parallel execution. In this case, parallel execution would be multiple threads reading the same file in different places, meaning the entire file can be read much quicker than with a single process. Additionally, I learned how important mutexes are for synchronization and the issues that can arise from improper synchronization. There must be some sort of issue in programs 4 and 5 regarding either using mutexes or communicating between the child process and parent process because these programs do not produce the correct output. At first program 3 did not work consistenly either, but through relocating where the pthread_mutex_lock() and pthread_mutex_unlock() functions get called, I was able to get that program to work consistently.  

Overall, I would say this was a good project that helped me learn a lot about processes, threads, synchronization, and more about the C programming language in general.
